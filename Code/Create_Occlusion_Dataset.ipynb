{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233add62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# 3rd party libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "## PyTorch\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "## our own modules\n",
    "from utils import get_percentage_of_image, normalize_image, generate_adversarial_examples, create_image_dict, read_conf_from_dotenv, get_dir_path\n",
    "\n",
    "conf = read_conf_from_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c799d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for dataset and output paths\n",
    "BASE_DIR = conf.BASE_DIR\n",
    "DATASET = \"atsds_large\"\n",
    "DATASET_SPLIT = \"test\"\n",
    "\n",
    "# Paths for dataset and associated outputs\n",
    "GROUND_TRUTH_DIR = get_dir_path(BASE_DIR, f\"{DATASET}_mask\", DATASET_SPLIT)\n",
    "BACKGROUND_DIR = get_dir_path(BASE_DIR, f\"{DATASET}_background\", DATASET_SPLIT)\n",
    "DATASET_DIR = get_dir_path(BASE_DIR, DATASET, DATASET_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef12beec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00001', '00002', '00003', '00004', '00005', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00017', '00018', '00025', '00031', '00035', '00038']\n"
     ]
    }
   ],
   "source": [
    "# Create folder structure for Occlusion Dataset\n",
    "CATEGORIES, imagedict = create_image_dict(BASE_DIR, DATASET, DATASET_SPLIT)\n",
    "print(CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84421abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"simple_cnn\"]\n",
    "xai_methods = [\"gradcam\", \"prism\", \"lime\", \"xrai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435818d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "occlusion_condition = lambda adv_mask: adv_mask == 0\n",
    "for model in models:\n",
    "    \n",
    "    for xaimethod in xai_methods:\n",
    "\n",
    "        XAI_DIR = get_dir_path(BASE_DIR, \"XAI_results\", model, xaimethod, DATASET_SPLIT, check_exists=False)\n",
    "        ADV_FOLDER = get_dir_path(BASE_DIR, \"XAI_evaluation\", model, xaimethod, \"occlusion\", check_exists=False)\n",
    "        occlusion_condition = lambda adv_mask: adv_mask == 0\n",
    "\n",
    "        generate_adversarial_examples(\n",
    "            adv_folder=ADV_FOLDER,\n",
    "            pct_range=range(10, 101, 10),\n",
    "            categories=CATEGORIES,\n",
    "            imagedict=imagedict,\n",
    "            img_path=DATASET_DIR,\n",
    "            background_dir=BACKGROUND_DIR,\n",
    "            xai_dir=XAI_DIR,\n",
    "            mask_condition=occlusion_condition\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c5ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pjoin = os.path.join\n",
    "\n",
    "im1_raw = np.load(pjoin(XAI_DIR, \"00001/mask/000035.png.npy\"))\n",
    "\n",
    "im2_raw = Image.open(pjoin(GROUND_TRUTH_DIR,  \"00001/000035.png\"))\n",
    "im3_raw = Image.open(pjoin(BACKGROUND_DIR, \"00001/000035.png\"))\n",
    "im4_raw = Image.open(pjoin(DATASET_DIR, \"00001/000035.png\"))\n",
    "im1 = np.array(np.array(im1_raw))\n",
    "im4 = np.array(im4_raw)\n",
    "im3 = np.array(im3_raw)\n",
    "im2 = np.array(im2_raw)/255\n",
    "im2_gs = np.array(im2_raw.convert('L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35d7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "axes[0, 0].imshow(im1)\n",
    "axes[0, 0].axis('off')  # remove axis\n",
    "\n",
    "axes[0, 1].imshow(im2)\n",
    "axes[0, 1].axis('off')  # remove axis\n",
    "\n",
    "axes[1, 0].imshow(im3)\n",
    "axes[1, 0].axis('off')  # remove axis\n",
    "\n",
    "axes[1, 1].imshow(im4)\n",
    "axes[1, 1].axis('off')  # remove axis\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7583125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-01-15: executed until here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6284302",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_methods = [\"lime\",\"prism\",\"ig_fixpoints\",\"xrai\",\"gradcam\"]\n",
    "MODEL_TYPE = \"simple_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eaa19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lime 10\n",
      "lime 20\n",
      "lime 30\n",
      "lime 40\n",
      "lime 50\n",
      "lime 60\n",
      "lime 70\n",
      "lime 80\n",
      "lime 90\n",
      "lime 100\n",
      "prism 10\n",
      "prism 20\n",
      "prism 30\n",
      "prism 40\n",
      "prism 50\n",
      "prism 60\n",
      "prism 70\n",
      "prism 80\n",
      "prism 90\n",
      "prism 100\n",
      "ig_fixpoints 10\n",
      "ig_fixpoints 20\n",
      "ig_fixpoints 30\n",
      "ig_fixpoints 40\n",
      "ig_fixpoints 50\n",
      "ig_fixpoints 60\n",
      "ig_fixpoints 70\n",
      "ig_fixpoints 80\n",
      "ig_fixpoints 90\n",
      "ig_fixpoints 100\n",
      "xrai 10\n",
      "xrai 20\n",
      "xrai 30\n",
      "xrai 40\n",
      "xrai 50\n",
      "xrai 60\n",
      "xrai 70\n",
      "xrai 80\n",
      "xrai 90\n",
      "xrai 100\n",
      "gradcam 10\n",
      "gradcam 20\n",
      "gradcam 30\n",
      "gradcam 40\n",
      "gradcam 50\n",
      "gradcam 60\n",
      "gradcam 70\n",
      "gradcam 80\n",
      "gradcam 90\n",
      "gradcam 100\n"
     ]
    }
   ],
   "source": [
    "for xai_m in xai_methods:\n",
    "#MODEL_TYPE = \"convnext_tiny\"\n",
    "    BASE_DIR = \"data/\"\n",
    "    DATASET = \"atsds_large\"\n",
    "    DATASET_SPLIT = \"test\"\n",
    "    GROUND_TRUTH_DIR = BASE_DIR + DATASET + \"_mask/\" + DATASET_SPLIT\n",
    "    BACKGROUND_DIR = BASE_DIR + DATASET + \"_background/\" + DATASET_SPLIT\n",
    "    DATASET_DIR = BASE_DIR + DATASET + \"/\" + DATASET_SPLIT\n",
    "    XAI_DIR = BASE_DIR + \"auswertung/\" + MODEL_TYPE + \"/\" + xai_m + \"/\" + DATASET_SPLIT + \"/\"\n",
    "    ADV_FOLDER = BASE_DIR + \"auswertung/\" + MODEL_TYPE + \"/\" + xai_m + \"/occlusion/\"\n",
    "\n",
    "    for pct in range(10,101,10):\n",
    "        print(xai_m + \" \" + str(pct))\n",
    "        for cat in CATEGORIES:\n",
    "            if not os.path.isdir(ADV_FOLDER + str(pct) + \"/test/\" + cat + \"/\"):\n",
    "                os.makedirs(ADV_FOLDER + str(pct) + \"/test/\" + cat + \"/\")\n",
    "            images = imagedict[cat]\n",
    "            for imagename in images:\n",
    "                current_img = normalize_image(np.array(Image.open(IMAGES_PATH + cat  + \"/\" + imagename)))\n",
    "                current_background = normalize_image(np.array(Image.open(BACKGROUND_DIR + \"/\" + cat + \"/\" + imagename)))\n",
    "                xai_mask = np.load(XAI_DIR + cat + \"/grad_mask/\" + imagename + \".npy\")\n",
    "                #if (xai_mask.shape != (512,512)):\n",
    "                    # Convert numpy array to PyTorch tensor\n",
    "                #    xai_mask_tensor = torch.tensor(xai_mask, dtype=torch.float32)\n",
    "                    # Ensure the tensor has a batch dimension and channel dimension\n",
    "                #    if xai_mask_tensor.ndim == 2:  # If it's a single grayscale image\n",
    "                #        xai_mask_tensor = xai_mask_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "                #    elif xai_mask_tensor.ndim == 3:  # If it has channels\n",
    "                #        xai_mask_tensor = xai_mask_tensor.unsqueeze(0)  # Add batch dimension\n",
    "                    # Resize xai_mask_tensor to (512, 512) using bilinear interpolation\n",
    "                #    resized_xai_mask = F.interpolate(xai_mask_tensor, size=(512, 512), mode='bilinear', align_corners=False)\n",
    "                #    xai_mask = np.array(resized_xai_mask)\n",
    "                adv_mask = normalize_image(get_percentage_of_image(np.ones_like(current_img),xai_mask,(pct/10)))\n",
    "                adv_example = np.where(adv_mask == 0, current_img, current_background)\n",
    "                adv_example_save = Image.fromarray((adv_example*255).astype('uint8'))\n",
    "                adv_example_save.save(ADV_FOLDER + str(pct) + \"/test/\" + cat + \"/\" + imagename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6319725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ju_tm_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "89444909e7444bfbcfb1b1f3b19bc4dd1e30b170eb0d9de786aca711fa67a661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
